{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set OpenAI key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../../.secrets.ini')\n",
    "OPENAI_API_KEY = config['OPENAI']['OPENAI_API_KEY']\n",
    "YOUTUBE_KEY = config['YOUTUBE']['YOUTUBE_API_KEY']\n",
    "NAVER_CLIENT_ID = config['NAVER']['NAVER_CLIENT_ID']\n",
    "NAVER_CLIENT_SECRET = config['NAVER']['NAVER_CLIENT_SECRET']\n",
    "GOOGLE_SEARCH_KEY = config['GOOGLE']['GOOGLE_API_KEY']\n",
    "CSE_ID = config['GOOGLE']['CSE_ID']\n",
    "SERPAPI_API_KEY = config['SERPAPI']['SERPAPI_API_KEY']\n",
    "\n",
    "\n",
    "os.environ.update({'OPENAI_API_KEY': OPENAI_API_KEY})\n",
    "os.environ.update({'YOUTUBE_KEY': YOUTUBE_KEY})\n",
    "os.environ.update({'NAVER_CLIENT_ID': NAVER_CLIENT_ID})\n",
    "os.environ.update({'NAVER_CLIENT_SECRET': NAVER_CLIENT_SECRET})\n",
    "os.environ.update({'GOOGLE_SEARCH_KEY': GOOGLE_SEARCH_KEY})\n",
    "os.environ.update({'CSE_ID': CSE_ID})\n",
    "os.environ.update({'SERPAPI_API_KEY': SERPAPI_API_KEY})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import load_prompt, PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_plan_template = '''\n",
    "I want you to make a plan for answering the user.\n",
    "Think step by step and write down the plan.\n",
    "You can ask questions to the user to make a plan. \n",
    "Answer in korean.\n",
    "\n",
    "If you want to ask to the user, use the following format:\n",
    "<format>\n",
    "User: user input.\n",
    "Thought1: I need to ask to the user.\n",
    "Ask to user: the question to ask to the user.\n",
    "End of Plan\n",
    "</format>\n",
    "Don't make a Plan when you ask question to user.\n",
    "\n",
    "If you can make the plan, Use the following format:\n",
    "<format>\n",
    "User: user input.\n",
    "Thought1: the first thought.\n",
    "Thought2: the second thought.\n",
    "... (this Thought can repeat as many times as you want)\n",
    "Step1: the first step.\n",
    "Step2: the second step.\n",
    "... (this Step can repeat as many times as you want)\n",
    "End of Plan\n",
    "</format>\n",
    "\n",
    "Begin!\n",
    "<format>\n",
    "{chat_history}\n",
    "User: {user_input}\n",
    "Thought1: \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.llm.chain import BaseChain\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class QnAPlanChain(BaseChain):\n",
    "    def __init__(self, \n",
    "                qna_plan_template=None, \n",
    "                input_variables:List[str]=None,\n",
    "                qna_plan_template_path='../openai_skt/models/templates/qna_plan_prompt.json', \n",
    "                model='gpt-4', \n",
    "                verbose=False) -> None:\n",
    "        super().__init__(template=qna_plan_template, input_variables=input_variables, template_path=qna_plan_template_path, model=model, verbose=verbose)\n",
    "\n",
    "    def run(self, user_input:str=None, chat_history:List[List[str]]=None):\n",
    "        return super().run(user_input=user_input, chat_history=chat_history)\n",
    "    \n",
    "    async def arun(self, user_input:str=None, chat_history:List[List[str]]=None):\n",
    "        return await super().arun(user_input=user_input, chat_history=chat_history)\n",
    "\n",
    "    def parse_input(self, user_input:str=None, chat_history:List[List[str]]=None):\n",
    "        chat_history_text = \"\"\n",
    "        for chat in chat_history[-2:]:\n",
    "            chat_history_text += f\"User: {chat[0]}\\n\"\n",
    "            chat_history_text += f\"Agent: {chat[1]}\\n\"\n",
    "        return {\"chat_history\": chat_history_text, \"user_input\": user_input}\n",
    "\n",
    "    def parse_output(self, output:str=None):\n",
    "        if 'Ask to user: ' in output:\n",
    "            return output.split('Ask to user: ')[1].strip().split('\\n')[0].strip()\n",
    "        elif 'Step' in output:\n",
    "            plans = re.findall(r'Step\\d+: (?:\\n)?(.*?\\.)(?=\\s|$)', output)\n",
    "            return plans\n",
    "        else:\n",
    "            raise ValueError('Invalid output format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_plan_chain = QnAPlanChain(qna_plan_template, input_variables=['user_input', 'chat_history'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "I want you to make a plan for answering the user.\n",
      "Think step by step and write down the plan.\n",
      "You can ask questions to the user to make a plan. \n",
      "Answer in korean.\n",
      "\n",
      "If you want to ask to the user, use the following format:\n",
      "<format>\n",
      "User: user input.\n",
      "Thought1: I need to ask to the user.\n",
      "Ask to user: the question to ask to the user.\n",
      "End of Plan\n",
      "</format>\n",
      "Don't make a Plan when you ask question to user.\n",
      "\n",
      "If you can make the plan, Use the following format:\n",
      "<format>\n",
      "User: user input.\n",
      "Thought1: the first thought.\n",
      "Thought2: the second thought.\n",
      "... (this Thought can repeat as many times as you want)\n",
      "Step1: the first step.\n",
      "Step2: the second step.\n",
      "... (this Step can repeat as many times as you want)\n",
      "End of Plan\n",
      "</format>\n",
      "\n",
      "Begin!\n",
      "<format>\n",
      "User: 오늘 날씨가 어때?\n",
      "Agent: 현재 어느 지역에 계신가요?\n",
      "\n",
      "User: 서울\n",
      "Thought1: \n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "사용자가 서울에 있다고 했으므로, 서울의 날씨 정보를 찾아야 한다.\n",
      "Thought2: \n",
      "날씨 정보는 실시간으로 변하기 때문에, 가장 최신의 정보를 제공해야 한다.\n",
      "\n",
      "Step1: \n",
      "인터넷에서 서울의 최신 날씨 정보를 검색한다.\n",
      "Step2: \n",
      "검색된 날씨 정보를 바탕으로 사용자에게 답변을 준비한다.\n",
      "Step3: \n",
      "준비된 답변을 사용자에게 제공한다.\n",
      "End of Plan\n"
     ]
    }
   ],
   "source": [
    "user_input = '서울'\n",
    "result = qna_plan_chain.run(user_input=user_input, chat_history=chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['인터넷에서 서울의 최신 날씨 정보를 검색한다.',\n",
       " '검색된 날씨 정보를 바탕으로 사용자에게 답변을 준비한다.',\n",
       " '준비된 답변을 사용자에게 제공한다.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.append([user_input, result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_critic_template = '''\n",
    "You are the critic who determines if the current step is working.\n",
    "The answer options are as follows:\n",
    "<option>\n",
    "A. The plan is working. Go to the next step.\n",
    "B. The plan is not working. Perform this step again.\n",
    "</option>\n",
    "\n",
    "Use the following format:\n",
    "<format>\n",
    "User: user input.\n",
    "Plan1: the first plan.\n",
    "Obs1: obs1 is the result of executing against that plan1.\n",
    "Plan2: the second plan.\n",
    "Obs2: the second observation.\n",
    "... (this Plan can repeat as many times as you want)\n",
    "Final Thought: Thoughts on how the plan and its observations are going.\n",
    "Final Answer: A or B\n",
    "</format>\n",
    "\n",
    "Begin!\n",
    "<format>\n",
    "{chat_history}\n",
    "User: {user_input}\n",
    "{plan_and_obs}\n",
    "Final Thought: \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QnACriticChain(BaseChain):\n",
    "    def __init__(self, \n",
    "                qna_critic_template=None, \n",
    "                input_variables:List[str]=None,\n",
    "                qna_critic_template_path='../openai_skt/models/templates/qna_critic_prompt.json', \n",
    "                model='gpt-4', \n",
    "                verbose=False) -> None:\n",
    "        super().__init__(template=qna_critic_template, input_variables=input_variables, template_path=qna_critic_template_path, model=model, verbose=verbose)\n",
    "\n",
    "    def run(self, user_input:str=None, plan_and_obs:List=None, chat_history:List[List[str]]=None):\n",
    "        return super().run(user_input=user_input, plan_and_obs=plan_and_obs, chat_history=chat_history)\n",
    "    \n",
    "    async def arun(self, user_input:str=None, plan_and_obs:List[List[str]]=None, chat_history:List[List[str]]=None):\n",
    "        return await super().arun(user_input=user_input, plan_and_obs=plan_and_obs, chat_history=chat_history)\n",
    "\n",
    "    def parse_input(self, user_input:str=None, plan_and_obs:List[List[str]]=None, chat_history:List[List[str]]=None):\n",
    "        chat_history_text = \"\"\n",
    "        for user, agent in chat_history[-3:]:\n",
    "            chat_history_text += f\"User: {user}\\n\"\n",
    "            chat_history_text += f\"Agent: {agent}\\n\"\n",
    "\n",
    "        plan_and_obs_text = \"\"\n",
    "        for idx, (plan, obs) in enumerate(plan_and_obs):\n",
    "            plan_and_obs_text += f\"Plan{idx+1}: {plan}\\n\"\n",
    "            plan_and_obs_text += f\"Obs{idx+1}: {obs}\\n\"\n",
    "        return {\"chat_history\": chat_history_text, \"user_input\": user_input, \"plan_and_obs\": plan_and_obs_text}\n",
    "    \n",
    "    def parse_output(self, output:str=None):\n",
    "        return output.split('Final Answer: ')[1][0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_critic_chain = QnACriticChain(qna_critic_template, input_variables=['user_input', 'chat_history', 'plan_and_obs'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_and_obs = [['인터넷에서 서울의 최신 날씨 정보를 검색한다.', '서울은 현재 25도 입니다.'], ['검색된 날씨 정보를 바탕으로 사용자에게 답변을 준비한다.', '서울의 현재 날씨는 21도 입니다.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are the critic who determines if the current step is working.\n",
      "The answer options are as follows:\n",
      "<option>\n",
      "A. The plan is working. Go to the next step.\n",
      "B. The plan is not working. Perform this step again.\n",
      "</option>\n",
      "\n",
      "Use the following format:\n",
      "<format>\n",
      "User: user input.\n",
      "Plan1: the first plan.\n",
      "Obs1: obs1 is the result of executing against that plan1.\n",
      "Plan2: the second plan.\n",
      "Obs2: the second observation.\n",
      "... (this Plan can repeat as many times as you want)\n",
      "Final Thought: Thoughts on how the plan and its observations are going.\n",
      "Final Answer: A or B\n",
      "</format>\n",
      "\n",
      "Begin!\n",
      "<format>\n",
      "User: 오늘 날씨가 어때?\n",
      "Agent: 현재 어느 지역에 계신가요?\n",
      "\n",
      "User: 서울\n",
      "Plan1: 인터넷에서 서울의 최신 날씨 정보를 검색한다.\n",
      "Obs1: 서울은 현재 25도 입니다.\n",
      "Plan2: 검색된 날씨 정보를 바탕으로 사용자에게 답변을 준비한다.\n",
      "Obs2: 서울의 현재 날씨는 21도 입니다.\n",
      "\n",
      "Final Thought: \n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = qna_critic_chain.run(user_input=user_input, plan_and_obs=plan_and_obs, chat_history=chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
